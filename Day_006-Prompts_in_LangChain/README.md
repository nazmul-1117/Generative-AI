# Day_006 | ‚úçÔ∏è Prompts in LangChain

In LangChain, **Prompts** are the components responsible for formatting and managing the input that is sent to a Large Language Model (LLM) or ChatModel. They are essential for turning raw user input and context into a structured, clear instruction set for the model.

-----

### LangChain Messages: The Foundation

LangChain uses a message-based structure to communicate with modern ChatModels. Instead of a single text string, the input is a list of objects, each with a specific role, which provides crucial context to the model. The main message types are:

  * **`SystemMessage`:** Sets the **behavior, role, and constraints** for the model for the entire conversation. (e.g., "You are a helpful travel assistant who only speaks French.")
  * **`HumanMessage`:** Represents the input from the human user.
  * **`AIMessage`:** Represents the output generated by the AI model.
  * **`FunctionMessage`/`ToolMessage`:** Used when the model calls an external function or tool, and this message carries the result of that execution back to the model for further reasoning.

### Single Message vs. List of Messages

| Feature | Single Message (Simple String) | List of Messages (Chat Format) |
| :--- | :--- | :--- |
| **Model Type** | Used with older **LLMs** (Text Completion models). | Used with modern **ChatModels**. |
| **Context** | All context (system instructions, history, user query) must be concatenated into a single, long string. | Context is separated by message role, giving the model clear structural cues. |
| **Conversations** | Requires **Memory** components to manage and insert conversation history explicitly. | Natively handles turn-taking, making it the superior choice for building chatbots. |
| **Recommended?** | **No**, generally discouraged for new development. | **Yes**, highly recommended for robust applications. |

-----

## üîÄ Static vs. Dynamic Prompts

### 1\. Static Prompts

A static prompt is a fixed string of text with **no variables** or placeholders. The exact text is sent to the LLM every time.

  * **Example:** `"Summarize the following text in exactly three bullet points."`
  * **Use Case:** Simple, fixed tasks like classification or basic instructions where the input content is separate.

### 2\. Dynamic Prompts (Prompt Templates)

A dynamic prompt uses a **template** structure containing one or more placeholders (variables) that are filled in at runtime based on the application's context or user input. LangChain's `PromptTemplate` is the component used for this.

  * **Example:** `"You are an expert {role}. Summarize this document: {text_input}"`
  * **Use Case:** Nearly all modern LLM applications, including RAG (where the `{text_input}` is filled with retrieved documents) and general-purpose chatbots.

-----

## üõë Why LangChain Prompts Over Python f-strings

While you *can* use standard Python f-strings to format your prompts, LangChain's `PromptTemplate` offers significant advantages for LLM application development:

| Feature | LangChain `PromptTemplate` | Python f-strings (`f"..."`) |
| :--- | :--- | :--- |
| **Placeholders** | Explicitly lists and validates input variables (e.g., `input_variables=['role', 'text_input']`). | No built-in validation; typos lead to runtime errors. |
| **Serialization** | Can be **serialized** (saved to a JSON or YAML file) and loaded back, making prompts easy to manage, version control, and share across teams. | Not easily serializable; tied to Python code. |
| **Composability** | Acts as a **Runnable**, allowing it to be easily integrated into a **Chain** using the LangChain Expression Language (LCEL) pipe operator (`|`). | Requires manual execution and integration into the LLM call logic. |
| **Model Specificity**| Easily accommodates the message list format required by ChatModels (`ChatPromptTemplate`). | Only handles string concatenation, requiring manual formatting for ChatModels. |

-----

## üßÆ Message Placeholders: `MessagesPlaceholder`

In complex LangChain applications, especially those involving **Memory** or **Agents**, a simple `PromptTemplate` is often insufficient. This is where the **`MessagesPlaceholder`** is used within a `ChatPromptTemplate`.

  * **What it is:** A specialized placeholder that accepts and inserts a *list of `Message` objects* (not just a single string) directly into the main prompt list.
  * **Why it's needed:**
      * **Memory Integration:** It's used to insert the **full conversation history** (managed by a LangChain Memory component) into the prompt list without losing the role metadata (`HumanMessage`, `AIMessage`).
      * **Agent Scratchpad:** In agent workflows, it's used to insert the **Agent Scratchpad**‚Äîthe history of tool-use actions and observations‚Äîinto the prompt list so the LLM can see its past steps before deciding the next one.

**Example `ChatPromptTemplate` Structure with Placeholder:**

```python
template = ChatPromptTemplate.from_messages([
    SystemMessage(content="You are an expert historian."),
    MessagesPlaceholder(variable_name="history"), # <-- Placeholder for chat history or agent steps
    HumanMessagePromptTemplate.from_template("{user_input}")
])
```

---


## ‚úÖ **1. Prompts in LangChain**

In LangChain, a *prompt* is the structured input you send to an LLM.
LangChain formalizes prompts so they are:

* reusable
* composable
* safe
* template-driven
* easy to serialize/store

LangChain prompts are usually created through **PromptTemplate** (for text) or **ChatPromptTemplate** (for chat-style models).

---

## ‚úÖ **2. Static vs Dynamic Prompts**

## **Static Prompt**

A static prompt has *no variables*.
It is always the same.

```python
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    "Explain quantum computing in simple terms."
)
```

**Use case:** fixed instructions, system prompts.

---

## **Dynamic Prompt**

A dynamic prompt contains **template variables** that are filled in at runtime.

```python
prompt = PromptTemplate.from_template(
    "Explain {topic} in simple terms."
)

prompt.format(topic="black holes")
```

**Use case:** Q&A apps, agents, RAG retrieval, user-driven tasks.

---

## ‚ùì Why not just use Python f-strings?

### ‚úî 1. **Prompt safety & injection protection**

LangChain escapes variables safely; f-strings do not.

### ‚úî 2. **Validation**

You can validate that expected variables exist.

### ‚úî 3. **Separation of logic & prompt**

Prompts can be stored in files, versioned, modified by non-developers.

### ‚úî 4. **Composable prompt chains**

You can plug prompts into chains, LLMs, tools, memory, agents.

### ‚úî 5. **Better debugging**

You can see the rendered prompt at any step.

### ‚úî 6. **Supports partials & message placeholders**

(f-strings cannot do this)

---

## ‚úÖ **3. Single message vs list of messages**

LLMs can work in **text completion** mode or **chat mode**.

---

### **Single Message Prompt**

Used with completion models (or simple cases with chat models).
It‚Äôs one text blob.

```python
prompt = PromptTemplate.from_template(
    "Write a summary of: {text}"
)
```

---

### **List of Messages (ChatPromptTemplate)**

Chat models expect **multiple messages** with roles (`system`, `user`, `assistant`).

```python
from langchain.prompts import ChatPromptTemplate

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    ("user", "Explain {topic} to me.")
])
```

**Why use multiple messages?**

* Gives structure to the LLM
* Allows injecting system behavior
* Works better with most modern LLMs (GPT, Claude, Gemini)
* Supports memory and agent frameworks

---

## ‚úÖ **4. Message placeholders**

Message placeholders allow you to insert **dynamic message lists**‚Äîe.g., memory, retrieved documents, prior chat history.

Example:

```python
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant."),
    MessagesPlaceholder("chat_history"),   # <-- placeholder
    ("user", "Answer the question: {question}")
])
```

At runtime:

```python
prompt.format_messages(
    chat_history=[
        ("user", "Hi"),
        ("assistant", "Hello! How can I help?")
    ],
    question="What is AI?"
)
```

**Use cases:**

* Conversation memory
* Multi-document RAG context
* Tool results inserted as messages

---

## ‚úÖ Summary Table

| Concept                        | What It Means                          | Why It Matters                          |
| ------------------------------ | -------------------------------------- | --------------------------------------- |
| **Static Prompt**              | No variables                           | For fixed instructions                  |
| **Dynamic Prompt**             | Variables in template                  | Customizable, flexible                  |
| **PromptTemplate vs f-string** | Safe templating wrapper                | Safety, validation, storage, reuse      |
| **Single Message**             | One text input                         | Simple tasks                            |
| **List of Messages**           | Structured chat roles                  | Better for chat models                  |
| **Message Placeholder**        | Dynamic insertion of multiple messages | Memory, document context, agent results |

---